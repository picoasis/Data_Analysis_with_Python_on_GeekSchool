# 决策树——CART

1. CART全称

   Classify and Regress Tree，分类回归树

2. 作为分类算法，CART与ID3 ，C4.5的区别

   | 算法       | 分类依据：属性                                   |
   | :--------- | ------------------------------------------------ |
   | ID3        | 信息增益，最大的作为根节点                       |
   | C4.5       | 信息增益率(信息增益/属性熵)，最大的作为根节点    |
   | CART分类树 | 基尼系数（样本的不确定度），选取最小的作为根节点 |
   | CART回归树 | 偏差（均值差，方差）                             |

3. CART分类树和回归树

   |              | CART分类树                        | CART回归树                         |
   | ------------ | --------------------------------- | ---------------------------------- |
   | 目标值       | 类别，用离散值标识                | 数值，用连续值记录                 |
   | 节点划分标准 | GINI系数                          | 最小绝对偏差LAD，或最小二乘偏差LSD |
   | 工具         | sklearn.tree.DecisionTreeClassify | sklearn.tree.DecisionTreeRegressor |



4. CART的剪枝方法：CCP  

| CCP      | cost-complexity prune | 代价复杂度，后剪枝方法                                       |
| -------- | --------------------- | ------------------------------------------------------------ |
| 关键参数 | 表面误差率增益值      | 节点t的子树被剪枝后的误差变化除以剪掉的叶子数量              |
| 步骤1    | 生成子树序列          | 基于表面误差率增益值，来判断剪枝前后的误差，找到最小误差值对应的节点，进行删除，从而生成子树序列 |
| 步骤2    | 取最优决策树          | 通过验证集，对所有子树的误差计算一遍，通过计算各个子树的基尼指数或者平方误差，其中误差最小的那个树，即为最优的决策树 |

